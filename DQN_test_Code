import pickle
import numpy as np
import torch
from torch import nn
import torch.nn.functional as F
from torch.distributions import Categorical
import os
import gym
import time
import matplotlib.pyplot as plt

"""
class Net(nn.Module):
    def __init__(self, n_states=5, n_actions=9, n_hidden=128):
        super(Net, self).__init__()

        # Two fully-connected layers, input (state) to hidden & hidden to output (action)
        self.fc1 = nn.Linear(n_states, n_hidden)
        self.out = nn.Linear(n_hidden, n_actions)

    def forward(self, x):               #x為當前狀態
        x = self.fc1(x)                 
        x = F.sigmoid(x)                   #連接輸入層到隱藏層，並使用激勵函數ReLU
        actions_value = self.out(x)     #連接隱藏層到輸出層，並輸出動作
        return actions_value
"""

class Net(nn.Module):
    def __init__(self, n_states=6, n_actions=9, n_hidden=200, n_hidden2=100):
        super(Net, self).__init__()

        # Two fully-connected layers, input (state) to hidden & hidden to output (action)
        self.fc1 = nn.Linear(n_states, n_hidden)
        self.fc2 = nn.Linear(n_hidden, n_hidden2)
        self.out = nn.Linear(n_hidden2, n_actions)

    def forward(self, x):               #x為當前狀態
        x1 = self.fc1(x)
        h1 = F.sigmoid(x1) 
        x2 = self.fc2(h1)                 
        h2 = F.sigmoid(x2)                    #連接輸入層到隱藏層，並使用激勵函數ReLU
        actions_value = self.out(h2)     #連接隱藏層到輸出層，並輸出動作
        return actions_value

for run in range(1):
    model = Net()
    model_location = "8000_E700_sigmoid_200x100_v5_test1_8.pth"
    #+str(run)+".pth"

    env = gym.make('PepperMap-v5')
    env = env.unwrapped # For cheating mode to access values hidden in the environment
    print()
    print("第" +str(run+1)+ "次測試")
    """
    #確認檔案是否存在
    if os.path.isfile(model_location):
        print("yes")
    else:
        print("no")    
    """
    weights = torch.load(model_location, map_location=lambda storage, loc: storage)

    model.load_state_dict(weights)
    model.eval()
    w_list = [i for i in weights]

    act = 0
    c1 = 0
    c2 = 0
    c3 = 0
    c4 = 0             
    c5 = 0
    c11 = 0
    c22 = 0
    c33 = 0
    c44 = 0             
    c55 = 0
    for i_episode in range(100):     #迭代
        time1 = time.time()
        t = 0 # timestep
        rewards = 0 # accumulate rewards for each episode
        total_reward = 0
        good = 0
        #state = env.reset() # reset environment to initial state for each episode
        #載入訓練case(每個case都跑20次)
        state = env.testCase(i_episode)
        while True:
            env.render()
            state = torch.unsqueeze(torch.FloatTensor(state), 0)
            action1 = model(state)
            action = torch.max(action1, 1)[1].data.numpy()[0]
            next_state, rewards, done ,info= env.step(action) # do the action, get the reward


            if rewards > 0:
                #判定是不是正向獎勵
                good += 1

            total_reward += rewards          
            #time.sleep(3)

            #判定各case的成功率
            if i_episode < 20:
                if done and rewards > 0:
                    c1 += 1
            elif i_episode >= 20 and i_episode < 40:
                if done and rewards > 0:
                    c2 += 1
            elif i_episode >= 40 and i_episode < 60:
                if done and rewards > 0:
                    c3 += 1
            elif i_episode >= 60 and i_episode < 80:
                if done and rewards > 0:
                    c4 += 1
            elif i_episode >= 80 and i_episode < 100:
                if done and rewards > 0:
                    c5 += 1
            
            #狀態轉移
            state = next_state

            #判斷是否達到結束條件
            
            if done and rewards > 0:
                #print('Episode {} finished after {} timesteps, total rewards {}'.format(i_episode, t+1, total_reward))
                time2 = time.time()
                UseTime =  time2 - time1
                
                if i_episode < 20:
                    if done and rewards > 0:
                        c11 += UseTime
                elif i_episode >= 20 and i_episode < 40:
                    if done and rewards > 0:
                        c22 += UseTime
                elif i_episode >= 40 and i_episode < 60:
                    if done and rewards > 0:
                        c33 += UseTime
                elif i_episode >= 60 and i_episode < 80:
                    if done and rewards > 0:
                        c44 += UseTime
                elif i_episode >= 80 and i_episode < 100:
                    if done and rewards > 0:
                        c55 += UseTime
                print(UseTime)

                act += 1
                #print('Episode {} finished after {} timesteps, total rewards {}'.format(i_episode, t+1, total_reward))
                break
            #time.sleep(0.5)
            
            t += 1
            if t > 100:
                #print(state)
                #print('Episode {} finished after {} timesteps, total rewards {}'.format(i_episode, t+1, total_reward))
                time2 = time.time()
                UseTime =  time2 - time1
                
                #print(UseTime)
                break
        #pt_loss[i_episode] = global_loss
    
    print ("Case 1 成功數 : " + str(c1) + "成功率 : " + str(c11 / c1))
    print ("Case 2 成功數 : " + str(c2) + "成功率 : " + str(c22 / c2))
    print ("Case 3 成功數 : " + str(c3) + "成功率 : " + str(c33 / c3))
    print ("Case 4 成功數 : " + str(c4) + "成功率 : " + str(c44 / c4))
    print ("Case 5 成功數 : " + str(c5) + "成功率 : " + str(c55 / c5))
    print ("總成功率 : " + str(act/10))
    env.close()
    g = 0
    for i in range(1000):
        g += plt_good[i] 
    g = g / 1000
    print("正向獎勵比例"+str(g))
